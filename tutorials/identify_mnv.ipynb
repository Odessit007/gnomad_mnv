{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "In this notebook, we will learn how to use hail functions to identify MNVs in a given vcf or matrixtable.\n",
    "\n",
    "Most of the work in this paper is performed in google cloud using hail. Specifically, we ran \n",
    "```\n",
    "cluster start mycluster --num-preemptible-workers 8 --vep --init gs://gnomad-public/tools/inits/master-init.sh\n",
    "```\n",
    "Followed by \n",
    "```\n",
    "cluster connect mycluster notebook\n",
    "```\n",
    "To build the jupyter notebook in google cloud cluster. \n",
    "\n",
    "After building the cluster and accessing this notebook, \n",
    "the first step is to import hail and relevant tools:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hail as hl\n",
    "import hail.expr.aggregators as agg\n",
    "from typing import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have trouble building the environment, we recommend to go through the [Hail intallation tutorials](https://hail.is/docs/0.2/getting_started.html). \n",
    "\n",
    "Also we use [cloudtools](https://github.com/Nealelab/cloudtools) for starting and managing the clusters.\n",
    "\n",
    "\n",
    "Now assuming that we have the environment built, \n",
    "first, let's start with an example of 1000genomes (subset 9831 variants in a chunk of chr1, in 2504 samples): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing Spark and Hail with default parameters...\n",
      "Running on Apache Spark version 2.2.3\n",
      "SparkUI available at http://10.128.0.44:4042\n",
      "Welcome to\n",
      "     __  __     <>__\n",
      "    / /_/ /__  __/ /\n",
      "   / __  / _ `/ / /\n",
      "  /_/ /_/\\_,_/_/_/   version 0.2.11-61a2083d5773\n",
      "LOGGING: writing to /home/hail/hail-20190309-2255-0.2.11-61a2083d5773.log\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        CIEND: array<int32>, \n",
      "        CIPOS: array<int32>, \n",
      "        CS: str, \n",
      "        END: int32, \n",
      "        IMPRECISE: bool, \n",
      "        MC: array<str>, \n",
      "        MEINFO: array<str>, \n",
      "        MEND: int32, \n",
      "        MLEN: int32, \n",
      "        MSTART: int32, \n",
      "        SVLEN: array<int32>, \n",
      "        SVTYPE: str, \n",
      "        TSD: str, \n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        NS: int32, \n",
      "        AN: int32, \n",
      "        EAS_AF: array<float64>, \n",
      "        EUR_AF: array<float64>, \n",
      "        AFR_AF: array<float64>, \n",
      "        AMR_AF: array<float64>, \n",
      "        SAS_AF: array<float64>, \n",
      "        DP: int32, \n",
      "        AA: str, \n",
      "        VT: array<str>, \n",
      "        EX_TARGET: bool, \n",
      "        MULTI_ALLELIC: bool\n",
      "    }\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vcf = hl.import_vcf(\"gs://gnomad-public/MNV/head100M_1000Genomes_chr1.vcf\", call_fields=[\"GT\"])\n",
    "vcf.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the imported vcf would naturally take the form of matrixtable in hail, \n",
    "let's re-write and read as a matrixtable (as in [hail tutorial](https://hail.is/docs/0.2/tutorials/01-genome-wide-association-study.html))\n",
    "\n",
    "Also, since we would like to split multi allelic sites for convenience, we are also applying `split_multi_hts` here.\n",
    "\n",
    "(Replace thr directory with your own, unless you are actually using `gnomad-qingbowang` as your favorite directory name.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:03:38 Hail: INFO: wrote matrix table with 9884 rows and 2504 columns in 2 partitions to gs://gnomad-qingbowang/MNV/head100M_1000Genomes_chr1.mt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'rsid': str\n",
      "    'qual': float64\n",
      "    'filters': set<str>\n",
      "    'info': struct {\n",
      "        CIEND: array<int32>, \n",
      "        CIPOS: array<int32>, \n",
      "        CS: str, \n",
      "        END: int32, \n",
      "        IMPRECISE: bool, \n",
      "        MC: array<str>, \n",
      "        MEINFO: array<str>, \n",
      "        MEND: int32, \n",
      "        MLEN: int32, \n",
      "        MSTART: int32, \n",
      "        SVLEN: array<int32>, \n",
      "        SVTYPE: str, \n",
      "        TSD: str, \n",
      "        AC: array<int32>, \n",
      "        AF: array<float64>, \n",
      "        NS: int32, \n",
      "        AN: int32, \n",
      "        EAS_AF: array<float64>, \n",
      "        EUR_AF: array<float64>, \n",
      "        AFR_AF: array<float64>, \n",
      "        AMR_AF: array<float64>, \n",
      "        SAS_AF: array<float64>, \n",
      "        DP: int32, \n",
      "        AA: str, \n",
      "        VT: array<str>, \n",
      "        EX_TARGET: bool, \n",
      "        MULTI_ALLELIC: bool\n",
      "    }\n",
      "    'a_index': int32\n",
      "    'was_split': bool\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vcf = hl.split_multi_hts(vcf)\n",
    "vcf.write('gs://gnomad-qingbowang/MNV/head100M_1000Genomes_chr1.mt', overwrite=True)\n",
    "mt = hl.read_matrix_table('gs://gnomad-qingbowang/MNV/head100M_1000Genomes_chr1.mt')\n",
    "mt.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "This toy example is easy, but in real application the matrixtable could be large. \n",
    "We will through away some unnecessary fields at this stage. \n",
    "(If you want to keep some more fields, change the lines as needed.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.select_cols() #dropping unneeded  columns makes things faster\n",
    "mt = mt.annotate_rows(AC = mt.info.AC[mt.a_index-1], AF = mt.info.AF[mt.a_index-1]) #for case of multiallelic\n",
    "mt = mt.select_rows(mt.filters, mt.AC, mt.AF) #or any rows that you want to store for future investigation\n",
    "\n",
    "mt = mt.filter_entries(hl.is_defined(mt.GT) & mt.GT.is_non_ref()) #interested in non-ref only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before calling MNV, let's inspect the data we are looking at:\n",
    "What we want in the `mt` data is, a pair of variants whose sample `s` is the same, and `GT` has a non_ref variant in same haplotype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+------+\n",
      "| locus         | alleles    | s         | GT   |\n",
      "+---------------+------------+-----------+------+\n",
      "| locus<GRCh37> | array<str> | str       | call |\n",
      "+---------------+------------+-----------+------+\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00096\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00097\" | 0|1  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00099\" | 0|1  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00100\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00101\" | 0|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00102\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00103\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00105\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00106\" | 1|0  |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00107\" | 0|0  |\n",
      "+---------------+------------+-----------+------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mt.GT.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to the `window_by_locus` function, this could be archieved relatively easily:\n",
    "\n",
    "## Running window_by_locus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = hl.window_by_locus(mt, 2) #partition in window -- we only care within codon reading frame, so the max distance is set to be 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see each entrie is tagged with `prev_entry`, which contains the information of the variant within the window range, \n",
    "in the same individual (if there is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Global fields:\n",
      "    None\n",
      "----------------------------------------\n",
      "Column fields:\n",
      "    's': str\n",
      "----------------------------------------\n",
      "Row fields:\n",
      "    'locus': locus<GRCh37>\n",
      "    'alleles': array<str>\n",
      "    'filters': set<str>\n",
      "    'AC': int32\n",
      "    'AF': float64\n",
      "    'prev_rows': array<struct {\n",
      "        locus: locus<GRCh37>, \n",
      "        alleles: array<str>, \n",
      "        filters: set<str>, \n",
      "        AC: int32, \n",
      "        AF: float64\n",
      "    }>\n",
      "----------------------------------------\n",
      "Entry fields:\n",
      "    'GT': call\n",
      "    'prev_entries': array<struct {\n",
      "        GT: call\n",
      "    }>\n",
      "----------------------------------------\n",
      "Column key: ['s']\n",
      "Row key: ['locus', 'alleles']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "mt.describe() #now we have this prev_rows / prev_entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+-------------------------+\n",
      "| locus         | alleles    | s         | prev_entries            |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "| locus<GRCh37> | array<str> | str       | array<struct{GT: call}> |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00096\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00097\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00099\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00100\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00101\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00102\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00103\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00105\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00106\" | []                      |\n",
      "| 1:10177       | [\"A\",\"AC\"] | \"HG00107\" | []                      |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mt.prev_entries.show() #This is how the prev_entry looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Actually most of the case there is no any prev_entries for a variant. So we can filter down the number a lot at this stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt = mt.filter_entries((mt.prev_entries.length() > 0))\n",
    "mt = mt.filter_entries((hl.is_defined(mt.GT) & (mt.prev_entries.length() > 0))) #throwing away no MNV SNPs\n",
    "mt = mt.filter_entries(mt.prev_entries.filter(lambda x: x.GT.is_non_ref()).length() > 0) #same\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can see the non null prev_entrie clearer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-----------+-------------------------+\n",
      "| locus         | alleles    | s         | prev_entries            |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "| locus<GRCh37> | array<str> | str       | array<struct{GT: call}> |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | \"HG02315\" | [(0|1)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00097\" | [(1|0)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00101\" | [(1|0)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00105\" | [(1|0)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00107\" | [(0|1)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00111\" | [(0|1)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00121\" | [(1|0)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00123\" | [(1|0)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00128\" | [(0|1)]                 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | \"HG00130\" | [(0|1)]                 |\n",
      "+---------------+------------+-----------+-------------------------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mt.prev_entries.show() #now we have the mnv candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to explode the matrixtable, and from here on we are going to work with the `table` format, \n",
    "a bit more intuitive for users working with pandas, r dataframe etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus         | alleles    | filters  |    AC |       AF | s         | GT   |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus<GRCh37> | array<str> | set<str> | int32 |  float64 | str       | call |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | {}       |     1 | 2.00e-04 | \"HG02315\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00097\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00101\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00105\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00107\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00111\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00121\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00123\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00128\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00130\" | 0|1  |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| prev_row.locus | prev_row.alleles | prev_row.filters | prev_row.AC | prev_row.AF |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| locus<GRCh37>  | array<str>       | set<str>         |       int32 |     float64 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| 1:10505        | [\"A\",\"T\"]        | {}               |           1 |    2.00e-04 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "\n",
      "+---------------+-------+\n",
      "| prev_entry.GT |  dist |\n",
      "+---------------+-------+\n",
      "| call          | int32 |\n",
      "+---------------+-------+\n",
      "| 0|1           |     1 |\n",
      "| 1|0           |     2 |\n",
      "| 1|0           |     2 |\n",
      "| 1|0           |     2 |\n",
      "| 0|1           |     2 |\n",
      "| 0|1           |     2 |\n",
      "| 1|0           |     2 |\n",
      "| 1|0           |     2 |\n",
      "| 0|1           |     2 |\n",
      "| 0|1           |     2 |\n",
      "+---------------+-------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#explode it to entries\n",
    "et = mt.key_cols_by().entries() # Matrix with 1000 rows (variant) + 1000 cols (sample)=> 1 million entries\n",
    "et = et.annotate(indices = hl.range(0, hl.len(et.prev_rows)))\n",
    "et = et.explode('indices') #for the case where there are more than one prev_row for a variant\n",
    "et = et.transmute(prev_row = et.prev_rows[et.indices],\n",
    "                      prev_entry = et.prev_entries[et.indices])#tracking back the one with matched indices\n",
    "et = et.annotate(dist=et.locus.position - et.prev_row.locus.position) #annotating the distance\n",
    "\n",
    "#filtering\n",
    "et = et.filter(et.dist>0) #distance=0 is just multiallelic\n",
    "et = et.filter( (et.alleles[0].length()==1) & (et.alleles[1].length()==1) \\\n",
    "                 & (et.prev_row.alleles[0].length()==1) & (et.prev_row.alleles[1].length()==1) )#interested in SNP only\n",
    "et = et.filter((et.filters.length()==0) & (et.prev_row.filters.length()==0)) #if you are interested in filter pass variants only\n",
    "\n",
    "et.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153304"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase inspection\n",
    "\n",
    "(See figure 1 a, b for the basic information)\n",
    "\n",
    "Now there are 153,304 variant pairs. Still a lot.\n",
    "Of course not all of them are MNV. Now let's look at MNVs. \n",
    "MNVs of 2 bp can be classified into 3 configurations (assuming diploid):\n",
    "\n",
    "1. Both are heterozygote\n",
    "2. One of them are heterozygote\n",
    "3. Both are homozygote\n",
    "\n",
    "In case of 2, and 3, we don't need to look at the phase information. 1 is homozygous MNV, and 2 is heterozygous MNV. \n",
    "The case 3 is a bit tricky, you actually need to look at the phase information to see whether they are in same haplotype (*cis*) or not.\n",
    "\n",
    "(Warning: In this example the variants do not have the phase ID `PID`, but in many cases there are, as in gnomAD. \n",
    "We recommend users to carefully define the right condition to call MNVs.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = et.annotate(hom = ((et.GT.is_diploid()) & (et.prev_entry.GT.is_diploid()) & et.GT.is_hom_var() & (et.prev_entry.GT.is_hom_var())),\n",
    "                 hethom = ((et.GT.is_diploid()) & (et.prev_entry.GT.is_diploid()) & (et.GT.is_hom_var() & et.prev_entry.GT.is_het_ref()) | (et.GT.is_het_ref() & et.prev_entry.GT.is_hom_var())),#including hom-het, just not distinguishing them two.\n",
    "                 hethet = ((et.GT.is_diploid()) & (et.prev_entry.GT.is_diploid()) & (et.GT.phased&et.prev_entry.GT.phased) & (et.GT.is_het_ref()&et.prev_entry.GT.is_het_ref()) & (et.GT==et.prev_entry.GT) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus         | alleles    | filters  |    AC |       AF | s         | GT   |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus<GRCh37> | array<str> | set<str> | int32 |  float64 | str       | call |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | {}       |     1 | 2.00e-04 | \"HG02315\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00097\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00101\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00105\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00107\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00111\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00121\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00123\" | 1|0  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00128\" | 0|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00130\" | 0|1  |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| prev_row.locus | prev_row.alleles | prev_row.filters | prev_row.AC | prev_row.AF |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| locus<GRCh37>  | array<str>       | set<str>         |       int32 |     float64 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| 1:10505        | [\"A\",\"T\"]        | {}               |           1 |    2.00e-04 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| prev_entry.GT |  dist |   hom | hethom | hethet |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| call          | int32 |  bool |   bool |   bool |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| 0|1           |     1 | false |  false |   true |\n",
      "| 1|0           |     2 | false |  false |   true |\n",
      "| 1|0           |     2 | false |  false |   true |\n",
      "| 1|0           |     2 | false |  false |   true |\n",
      "| 0|1           |     2 | false |  false |   true |\n",
      "| 0|1           |     2 | false |  false |   true |\n",
      "| 1|0           |     2 | false |  false |   true |\n",
      "| 1|0           |     2 | false |  false |   true |\n",
      "| 0|1           |     2 | false |  false |   true |\n",
      "| 0|1           |     2 | false |  false |   true |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:34:34 Hail: INFO: wrote table with 11721 rows in 2 partitions to gs://gnomad-qingbowang/MNV/demo_et_het.ht\n"
     ]
    }
   ],
   "source": [
    "#now you can for example look at the het het pairs\n",
    "et_het = et.filter(et.hethet)\n",
    "et_het.show()\n",
    "et_het.write(\"gs://gnomad-qingbowang/MNV/demo_et_het.ht\", overwrite=True) #and we are going to save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus         | alleles    | filters  |    AC |       AF | s         | GT   |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus<GRCh37> | array<str> | set<str> | int32 |  float64 | str       | call |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| 1:55165       | [\"A\",\"G\"]  | {}       |     1 | 2.00e-04 | \"NA18505\" | 1|0  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"HG01886\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"HG02571\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"HG03086\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"HG03115\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"NA19031\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"NA19095\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"NA19121\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"NA19141\" | 1|1  |\n",
      "| 1:362905      | [\"T\",\"G\"]  | {}       |   633 | 1.26e-01 | \"NA19152\" | 1|1  |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| prev_row.locus | prev_row.alleles | prev_row.filters | prev_row.AC | prev_row.AF |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| locus<GRCh37>  | array<str>       | set<str>         |       int32 |     float64 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| 1:55164        | [\"C\",\"A\"]        | {}               |        4624 |    9.23e-01 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "| 1:362904       | [\"T\",\"G\"]        | {}               |         154 |    3.08e-02 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| prev_entry.GT |  dist |   hom | hethom | hethet |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| call          | int32 |  bool |   bool |   bool |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "| 1|1           |     1 | false |   true |  false |\n",
      "| 0|1           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 0|1           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 1|0           |     1 | false |   true |  false |\n",
      "| 0|1           |     1 | false |   true |  false |\n",
      "+---------------+-------+-------+--------+--------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:34:58 Hail: INFO: wrote table with 1184 rows in 2 partitions to gs://gnomad-qingbowang/MNV/demo_et_hethom.ht\n"
     ]
    }
   ],
   "source": [
    "et_hethom = et.filter(et.hethom)\n",
    "et_hethom.show()\n",
    "et_hethom.write(\"gs://gnomad-qingbowang/MNV/demo_et_hethom.ht\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus         | alleles    | filters  |    AC |       AF | s         | GT   |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| locus<GRCh37> | array<str> | set<str> | int32 |  float64 | str       | call |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00261\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00265\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00328\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00382\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG00640\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG01085\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG01161\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG01372\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG01413\" | 1|1  |\n",
      "| 1:13118       | [\"A\",\"G\"]  | {}       |   486 | 9.70e-02 | \"HG01586\" | 1|1  |\n",
      "+---------------+------------+----------+-------+----------+-----------+------+\n",
      "\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| prev_row.locus | prev_row.alleles | prev_row.filters | prev_row.AC | prev_row.AF |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| locus<GRCh37>  | array<str>       | set<str>         |       int32 |     float64 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "| 1:13116        | [\"T\",\"G\"]        | {}               |         486 |    9.70e-02 |\n",
      "+----------------+------------------+------------------+-------------+-------------+\n",
      "\n",
      "+---------------+-------+------+--------+--------+\n",
      "| prev_entry.GT |  dist |  hom | hethom | hethet |\n",
      "+---------------+-------+------+--------+--------+\n",
      "| call          | int32 | bool |   bool |   bool |\n",
      "+---------------+-------+------+--------+--------+\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "| 1|1           |     2 | true |  false |  false |\n",
      "+---------------+-------+------+--------+--------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:35:35 Hail: INFO: wrote table with 11589 rows in 2 partitions to gs://gnomad-qingbowang/MNV/demo_et_hom.ht\n"
     ]
    }
   ],
   "source": [
    "et_hom = et.filter(et.hom)\n",
    "et_hom.show()\n",
    "et_hom.write(\"gs://gnomad-qingbowang/MNV/demo_et_hom.ht\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregation per variant\n",
    "In the analysis of the mechanisms and the impact in the population, what we want to know is the allele count of MNV, \n",
    "rather than the individuals who are carrying it. In this case, we can use aggregator in hail to get per variant statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_variant_het = et_het.group_by('locus', 'alleles', \"prev_row\").aggregate(n=hl.agg.count()) \n",
    "per_variant_hom = et_hom.group_by('locus', 'alleles', \"prev_row\").aggregate(n=hl.agg.count()) \n",
    "per_variant_hethom = et_hethom.group_by('locus', 'alleles', \"prev_row\").aggregate(n=hl.agg.count()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it is not easy to annotate back some columns that does not depend on the variant, for downstream analysis.\n",
    "(e.g. in this tutorial we threw away filtered variants, but in gnomAD analysis we kept them and compared the statistics between filtered and unfilted variants. Just for such case, we are annotating the `filters` column as well, although it is not directly used in this tutorial)\n",
    "\n",
    "(Supplementary Figure 11 was generated in such way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and we also rename some columns for convenience\n",
    "et_het = et_het.key_by(\"locus\", \"alleles\", \"prev_row\")\n",
    "per_variant_het = per_variant_het.annotate(dist = et_het[per_variant_het.key].dist,\n",
    "                                                  AF = et_het[per_variant_het.key].AF,\n",
    "                                                  AC = et_het[per_variant_het.key].AC,\n",
    "                                                  filters = et_het[per_variant_het.key].filters)\n",
    "per_variant_het = per_variant_het.annotate(prev_locus = per_variant_het.prev_row.locus,\n",
    "                                                  prev_alleles = per_variant_het.prev_row.alleles,\n",
    "                                                  prev_filters = per_variant_het.prev_row.filters,\n",
    "                                                  prev_AC = per_variant_het.prev_row.AC,\n",
    "                                                  prev_AF = per_variant_het.prev_row.AF)\n",
    "et_hom = et_hom.key_by(\"locus\", \"alleles\", \"prev_row\")\n",
    "per_variant_hom = per_variant_hom.annotate(dist = et_hom[per_variant_hom.key].dist,\n",
    "                                                  AF = et_hom[per_variant_hom.key].AF,\n",
    "                                                  AC = et_hom[per_variant_hom.key].AC,\n",
    "                                                  filters = et_hom[per_variant_hom.key].filters)\n",
    "per_variant_hom = per_variant_hom.annotate(prev_locus = per_variant_hom.prev_row.locus,\n",
    "                                                  prev_alleles = per_variant_hom.prev_row.alleles,\n",
    "                                                  prev_filters = per_variant_hom.prev_row.filters,\n",
    "                                                  prev_AC = per_variant_hom.prev_row.AC,\n",
    "                                                  prev_AF = per_variant_hom.prev_row.AF)\n",
    "et_hethom = et_hethom.key_by(\"locus\", \"alleles\", \"prev_row\")\n",
    "per_variant_hethom = per_variant_hethom.annotate(dist = et_hethom[per_variant_hethom.key].dist,\n",
    "                                                  AF = et_hethom[per_variant_hethom.key].AF,\n",
    "                                                  AC = et_hethom[per_variant_hethom.key].AC,\n",
    "                                                  filters = et_hethom[per_variant_hethom.key].filters)\n",
    "per_variant_hethom = per_variant_hethom.annotate(prev_locus = per_variant_hethom.prev_row.locus,\n",
    "                                                  prev_alleles = per_variant_hethom.prev_row.alleles,\n",
    "                                                  prev_filters = per_variant_hethom.prev_row.filters,\n",
    "                                                  prev_AC = per_variant_hethom.prev_row.AC,\n",
    "                                                  prev_AF = per_variant_hethom.prev_row.AF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:40:46 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:40:54 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:02 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:10 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:18 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:26 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:34 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:42 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:41:50 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| locus         | alleles    | prev_row.locus | prev_row.alleles | prev_row.filters |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| locus<GRCh37> | array<str> | locus<GRCh37>  | array<str>       | set<str>         |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | 1:10505        | [\"A\",\"T\"]        | {}               |\n",
      "| 1:13118       | [\"A\",\"G\"]  | 1:13116        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:48328       | [\"A\",\"T\"]  | 1:48327        | [\"C\",\"A\"]        | {}               |\n",
      "| 1:49989       | [\"A\",\"G\"]  | 1:49988        | [\"T\",\"A\"]        | {}               |\n",
      "| 1:51049       | [\"A\",\"C\"]  | 1:51047        | [\"A\",\"T\"]        | {}               |\n",
      "| 1:51050       | [\"A\",\"T\"]  | 1:51049        | [\"A\",\"C\"]        | {}               |\n",
      "| 1:55545       | [\"C\",\"T\"]  | 1:55544        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:57264       | [\"T\",\"G\"]  | 1:57262        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:74792       | [\"G\",\"A\"]  | 1:74790        | [\"C\",\"G\"]        | {}               |\n",
      "| 1:77502       | [\"A\",\"G\"]  | 1:77501        | [\"C\",\"T\"]        | {}               |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "\n",
      "+-------------+-------------+-------+-------+----------+-------+----------+\n",
      "| prev_row.AC | prev_row.AF |     n |  dist |       AF |    AC | filters  |\n",
      "+-------------+-------------+-------+-------+----------+-------+----------+\n",
      "|       int32 |     float64 | int64 | int32 |  float64 | int32 | set<str> |\n",
      "+-------------+-------------+-------+-------+----------+-------+----------+\n",
      "|           1 |    2.00e-04 |     1 |     1 | 2.00e-04 |     1 | {}       |\n",
      "|         486 |    9.70e-02 |   414 |     2 | 9.70e-02 |   486 | {}       |\n",
      "|          22 |    4.39e-03 |    22 |     1 | 4.39e-03 |    22 | {}       |\n",
      "|          29 |    5.79e-03 |    29 |     1 | 5.79e-03 |    29 | {}       |\n",
      "|           8 |    1.60e-03 |     8 |     2 | 1.60e-03 |     8 | {}       |\n",
      "|           8 |    1.60e-03 |     8 |     1 | 1.60e-03 |     8 | {}       |\n",
      "|           1 |    2.00e-04 |     1 |     1 | 2.39e-01 |  1198 | {}       |\n",
      "|           7 |    1.40e-03 |     7 |     2 | 1.40e-03 |     7 | {}       |\n",
      "|         189 |    3.77e-02 |   187 |     2 | 3.77e-02 |   189 | {}       |\n",
      "|           4 |    7.99e-04 |     4 |     1 | 7.99e-04 |     4 | {}       |\n",
      "+-------------+-------------+-------+-------+----------+-------+----------+\n",
      "\n",
      "+---------------+--------------+--------------+---------+----------+\n",
      "| prev_locus    | prev_alleles | prev_filters | prev_AC |  prev_AF |\n",
      "+---------------+--------------+--------------+---------+----------+\n",
      "| locus<GRCh37> | array<str>   | set<str>     |   int32 |  float64 |\n",
      "+---------------+--------------+--------------+---------+----------+\n",
      "| 1:10505       | [\"A\",\"T\"]    | {}           |       1 | 2.00e-04 |\n",
      "| 1:13116       | [\"T\",\"G\"]    | {}           |     486 | 9.70e-02 |\n",
      "| 1:48327       | [\"C\",\"A\"]    | {}           |      22 | 4.39e-03 |\n",
      "| 1:49988       | [\"T\",\"A\"]    | {}           |      29 | 5.79e-03 |\n",
      "| 1:51047       | [\"A\",\"T\"]    | {}           |       8 | 1.60e-03 |\n",
      "| 1:51049       | [\"A\",\"C\"]    | {}           |       8 | 1.60e-03 |\n",
      "| 1:55544       | [\"T\",\"G\"]    | {}           |       1 | 2.00e-04 |\n",
      "| 1:57262       | [\"T\",\"G\"]    | {}           |       7 | 1.40e-03 |\n",
      "| 1:74790       | [\"C\",\"G\"]    | {}           |     189 | 3.77e-02 |\n",
      "| 1:77501       | [\"C\",\"T\"]    | {}           |       4 | 7.99e-04 |\n",
      "+---------------+--------------+--------------+---------+----------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check how it looks like:\n",
    "per_variant_het.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the table is much cleaner. The final step is to combine the frequency of het, hethom and hom.\n",
    "This is also easily done in hail by using `join`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:42:51 Hail: INFO: Table.join: renamed the following fields on the right to avoid name conflicts:\n",
      "    'prev_row' -> 'prev_row_1'\n",
      "    'n' -> 'n_1'\n",
      "2019-03-09 23:42:51 Hail: INFO: Table.join: renamed the following fields on the right to avoid name conflicts:\n",
      "    'prev_row' -> 'prev_row_2'\n",
      "    'n' -> 'n_2'\n",
      "2019-03-09 23:43:41 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:43:50 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:43:58 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:07 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:15 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:23 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:31 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:39 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:44:47 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:45:07 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:45:14 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:45:21 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:45:29 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:45:36 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:45:44 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:45:52 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:46:00 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:46:08 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:46:16 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:47:34 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:47:41 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:47:48 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:47:54 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:48:01 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:48:08 Hail: INFO: Coerced sorted dataset\n",
      "2019-03-09 23:48:36 Hail: INFO: Coerced sorted dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+---------------+--------------+-------+----------+-------+\n",
      "| locus         | alleles    | prev_locus    | prev_alleles |  dist |       AF |    AC |\n",
      "+---------------+------------+---------------+--------------+-------+----------+-------+\n",
      "| locus<GRCh37> | array<str> | locus<GRCh37> | array<str>   | int32 |  float64 | int32 |\n",
      "+---------------+------------+---------------+--------------+-------+----------+-------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | 1:10505       | [\"A\",\"T\"]    |     1 | 2.00e-04 |     1 |\n",
      "| 1:13118       | [\"A\",\"G\"]  | 1:13116       | [\"T\",\"G\"]    |     2 | 9.70e-02 |   486 |\n",
      "| 1:48328       | [\"A\",\"T\"]  | 1:48327       | [\"C\",\"A\"]    |     1 | 4.39e-03 |    22 |\n",
      "| 1:49989       | [\"A\",\"G\"]  | 1:49988       | [\"T\",\"A\"]    |     1 | 5.79e-03 |    29 |\n",
      "| 1:51049       | [\"A\",\"C\"]  | 1:51047       | [\"A\",\"T\"]    |     2 | 1.60e-03 |     8 |\n",
      "| 1:51050       | [\"A\",\"T\"]  | 1:51049       | [\"A\",\"C\"]    |     1 | 1.60e-03 |     8 |\n",
      "| 1:55165       | [\"A\",\"G\"]  | 1:55164       | [\"C\",\"A\"]    |     1 | 2.00e-04 |     1 |\n",
      "| 1:55545       | [\"C\",\"T\"]  | 1:55544       | [\"T\",\"G\"]    |     1 | 2.39e-01 |  1198 |\n",
      "| 1:57264       | [\"T\",\"G\"]  | 1:57262       | [\"T\",\"G\"]    |     2 | 1.40e-03 |     7 |\n",
      "| 1:74792       | [\"G\",\"A\"]  | 1:74790       | [\"C\",\"G\"]    |     2 | 3.77e-02 |   189 |\n",
      "+---------------+------------+---------------+--------------+-------+----------+-------+\n",
      "\n",
      "+----------+----------+---------+--------------+----------+----------+----------+\n",
      "| filters  |  prev_AF | prev_AC | prev_filters | n_hethet | n_hethom | n_homhom |\n",
      "+----------+----------+---------+--------------+----------+----------+----------+\n",
      "| set<str> |  float64 |   int32 | set<str>     |    int64 |    int64 |    int64 |\n",
      "+----------+----------+---------+--------------+----------+----------+----------+\n",
      "| {}       | 2.00e-04 |       1 | {}           |        1 |        0 |        0 |\n",
      "| {}       | 9.70e-02 |     486 | {}           |      414 |       36 |        0 |\n",
      "| {}       | 4.39e-03 |      22 | {}           |       22 |        0 |        0 |\n",
      "| {}       | 5.79e-03 |      29 | {}           |       29 |        0 |        0 |\n",
      "| {}       | 1.60e-03 |       8 | {}           |        8 |        0 |        0 |\n",
      "| {}       | 1.60e-03 |       8 | {}           |        8 |        0 |        0 |\n",
      "| {}       | 9.23e-01 |    4624 | {}           |        0 |        0 |        1 |\n",
      "| {}       | 2.00e-04 |       1 | {}           |        1 |        0 |        0 |\n",
      "| {}       | 1.40e-03 |       7 | {}           |        7 |        0 |        0 |\n",
      "| {}       | 3.77e-02 |     189 | {}           |      187 |        1 |        0 |\n",
      "+----------+----------+---------+--------------+----------+----------+----------+\n",
      "\n",
      "+---------+\n",
      "| n_total |\n",
      "+---------+\n",
      "|   int64 |\n",
      "+---------+\n",
      "|       1 |\n",
      "|     450 |\n",
      "|      22 |\n",
      "|      29 |\n",
      "|       8 |\n",
      "|       8 |\n",
      "|       1 |\n",
      "|       1 |\n",
      "|       7 |\n",
      "|     188 |\n",
      "+---------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comb = per_variant_het.key_by(\"locus\", \"alleles\",\"prev_locus\",\"prev_alleles\",\"dist\",\"AF\",\"AC\",\"filters\",\"prev_AF\",\"prev_AC\",\"prev_filters\") \\\n",
    "            .join(per_variant_hom.key_by(\"locus\", \"alleles\", \"prev_locus\", \"prev_alleles\", \"dist\", \"AF\", \"AC\", \"filters\", \"prev_AF\",\"prev_AC\", \"prev_filters\"), how='outer') \\\n",
    "            .join(per_variant_hethom.key_by(\"locus\", \"alleles\",\"prev_locus\",\"prev_alleles\",\"dist\",\"AF\",\"AC\",\"filters\",\"prev_AF\",\"prev_AC\",\"prev_filters\"), how='outer')\n",
    "comb = comb.transmute(n_hethet=hl.or_else(comb.n, 0), n_hethom=hl.or_else(comb.n_1, 0), n_homhom=hl.or_else(comb.n_2, 0))\n",
    "comb = comb.select(\"n_hethet\",\"n_hethom\",\"n_homhom\")\n",
    "comb = comb.annotate(n_total = comb.n_hethet + comb.n_hethom + comb.n_homhom)\n",
    "comb.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## aggregation per sample\n",
    "\n",
    "If users are more interested in getting per sample statistics, it is possible to do so as well, using the same principle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-09 23:59:24 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-09 23:59:30 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "| s         |     n |\n",
      "+-----------+-------+\n",
      "| str       | int64 |\n",
      "+-----------+-------+\n",
      "| \"HG00096\" |     2 |\n",
      "| \"HG00097\" |     3 |\n",
      "| \"HG00099\" |     3 |\n",
      "| \"HG00100\" |     1 |\n",
      "| \"HG00101\" |     5 |\n",
      "| \"HG00103\" |     2 |\n",
      "| \"HG00105\" |     5 |\n",
      "| \"HG00106\" |     5 |\n",
      "| \"HG00107\" |     3 |\n",
      "| \"HG00108\" |     3 |\n",
      "+-----------+-------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_sample_het = et_het.group_by(\"s\").aggregate(n=hl.agg.count()) \n",
    "per_sample_het.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells that sample \"HG00096\" has 2 heterozygous MNVs, and so on.\n",
    "(Or in practice, you can join the `et_het`, `et_hom`, and `et_hethom` first, and then do the aggregation)\n",
    "(This kind of approach would be more interesting after annotating the functional impact of each MNV.)\n",
    "\n",
    "Alternatively, we can keep the sample names tagged with each MNV entries, if we are more interested in the individual samples:\n",
    "(Warning: since some MNVs have allele frequency ~=1, this could make your file very large depending on the sample size and allele frequency distributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-10 00:03:54 Hail: INFO: Ordering unsorted dataset with network shuffle\n",
      "2019-03-10 00:04:00 Hail: INFO: Ordering unsorted dataset with network shuffle\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| locus         | alleles    | prev_row.locus | prev_row.alleles | prev_row.filters |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| locus<GRCh37> | array<str> | locus<GRCh37>  | array<str>       | set<str>         |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "| 1:10506       | [\"C\",\"G\"]  | 1:10505        | [\"A\",\"T\"]        | {}               |\n",
      "| 1:13118       | [\"A\",\"G\"]  | 1:13116        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:48328       | [\"A\",\"T\"]  | 1:48327        | [\"C\",\"A\"]        | {}               |\n",
      "| 1:49989       | [\"A\",\"G\"]  | 1:49988        | [\"T\",\"A\"]        | {}               |\n",
      "| 1:51049       | [\"A\",\"C\"]  | 1:51047        | [\"A\",\"T\"]        | {}               |\n",
      "| 1:51050       | [\"A\",\"T\"]  | 1:51049        | [\"A\",\"C\"]        | {}               |\n",
      "| 1:55545       | [\"C\",\"T\"]  | 1:55544        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:57264       | [\"T\",\"G\"]  | 1:57262        | [\"T\",\"G\"]        | {}               |\n",
      "| 1:74792       | [\"G\",\"A\"]  | 1:74790        | [\"C\",\"G\"]        | {}               |\n",
      "| 1:77502       | [\"A\",\"G\"]  | 1:77501        | [\"C\",\"T\"]        | {}               |\n",
      "+---------------+------------+----------------+------------------+------------------+\n",
      "\n",
      "+-------------+-------------+-------+\n",
      "| prev_row.AC | prev_row.AF |     n |\n",
      "+-------------+-------------+-------+\n",
      "|       int32 |     float64 | int64 |\n",
      "+-------------+-------------+-------+\n",
      "|           1 |    2.00e-04 |     1 |\n",
      "|         486 |    9.70e-02 |   414 |\n",
      "|          22 |    4.39e-03 |    22 |\n",
      "|          29 |    5.79e-03 |    29 |\n",
      "|           8 |    1.60e-03 |     8 |\n",
      "|           8 |    1.60e-03 |     8 |\n",
      "|           1 |    2.00e-04 |     1 |\n",
      "|           7 |    1.40e-03 |     7 |\n",
      "|         189 |    3.77e-02 |   187 |\n",
      "|           4 |    7.99e-04 |     4 |\n",
      "+-------------+-------------+-------+\n",
      "\n",
      "+----------------------------------------------------------------------------------------+\n",
      "| s                                                                                      |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "| array<str>                                                                             |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "| [\"HG02315\"]                                                                            |\n",
      "| [\"HG00097\",\"HG00101\",\"HG00105\",\"HG00107\",\"HG00111\",\"HG00121\",\"HG00123\",\"HG00128\",\"H... |\n",
      "| [\"HG00671\",\"HG01063\",\"HG01915\",\"HG02051\",\"HG02339\",\"HG02646\",\"HG02820\",\"HG02879\",\"H... |\n",
      "| [\"HG00500\",\"HG00728\",\"HG01360\",\"HG02014\",\"HG02138\",\"HG02337\",\"HG02433\",\"HG02496\",\"H... |\n",
      "| [\"HG00178\",\"HG00282\",\"HG00353\",\"HG00361\",\"NA18995\",\"NA19056\",\"NA19065\",\"NA20763\"]      |\n",
      "| [\"HG00178\",\"HG00282\",\"HG00353\",\"HG00361\",\"NA18995\",\"NA19056\",\"NA19065\",\"NA20763\"]      |\n",
      "| [\"HG01862\"]                                                                            |\n",
      "| [\"HG02648\",\"HG02649\",\"HG03854\",\"HG03919\",\"HG04035\",\"NA20890\",\"NA21127\"]                |\n",
      "| [\"HG00099\",\"HG00106\",\"HG00267\",\"HG00331\",\"HG00350\",\"HG00364\",\"HG01075\",\"HG01092\",\"H... |\n",
      "| [\"HG00982\",\"HG02153\",\"NA18605\",\"NA18617\"]                                              |\n",
      "+----------------------------------------------------------------------------------------+\n",
      "showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "per_variant_het = et_het.group_by('locus', 'alleles', \"prev_row\").aggregate(n=hl.agg.count(), s=hl.agg.collect(et_het.s))\n",
    "per_variant_het.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End notes\n",
    "\n",
    "- This tutorial explained how the MNV discovery works. As a user, of course you do not really need to run through this pipeline step by step everytime, since we have prepared a function `get_mnv.py`, which by default writes per_variant files and other intermediate files. \n",
    "(You can also let the function write per individual stats, with the argument `per_indv=True`, or keep the sample info for the per variant statistics by `keep_samplenames=True`.)\n",
    "\n",
    "- However, since the use case of MNV call could be different from one lab to another, we have not made the function so flexible by default. \n",
    "(e.g. it does not look at `PID` by default, which is wrong if your data does have `PID`.)\n",
    "We hope the users to use the function as a prototype to exlore their own analysis. \n",
    "\n",
    "- (Also, we have not explained about MNV consisting of 3bp (or more). This is because we have learned that such MNV are extremely rare, and both the probability that it could be causal for phenotype of a single person and the effect on the population level statistics is limited. However, interested users can look at the `tnv_call.py` to apply such analysis.)\n",
    "\n",
    "- The next step is to annotate the MNVs, to see which MNV has interesting impact on the transcript. see you at `annotate_mnv.ipynb`.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hail",
   "language": "python",
   "name": "hail"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
